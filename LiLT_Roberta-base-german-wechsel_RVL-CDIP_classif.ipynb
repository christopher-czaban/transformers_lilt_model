{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e51a9d-0fd8-4030-8ca5-7630f59abf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296be2b0-60df-4405-8844-f5dc2f70a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3.4\n"
     ]
    }
   ],
   "source": [
    "print(pytesseract.get_tesseract_version())\n",
    "os.environ[\"TESSDATA_PREFIX\"] = \"/Users/christopherczaban/Local_Project_Files/OCR_Tests/tessdata\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c7e051-85b8-4ca1-bd91-584e199e5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherczaban/Local_Project_Files/HuggingFace_Models/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.processing_utils import ProcessorMixin\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n",
    "from transformers.utils import TensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c01e468-6d15-400e-b479-ee8e074460b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LiltForSequenceClassification\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
    "from typing import List,Dict, Iterable, Optional, Union\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52029aef-2351-4799-8de9-dab5d6d1cf4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42c997-b786-43eb-8254-2dab3217b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb45804-dc39-4686-9e1d-8884104d7e18",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65d9d8-b899-420f-83e2-71192596af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching\n",
    "\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd71f3d-9186-4f10-b981-c5b8e0be1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = {\n",
    "    \"base_dir\": \"/Users/christopherczaban/Local_Project_Files/Kundenprojekte_Code_lokal/Code_pma/Dokumente_Kategorisierung/Dokumente/PoV-Kategorie-Split_Erste_und_Weitere-Seiten-gemischt\",\n",
    "    \"train\": \"/Users/christopherczaban/Local_Project_Files/Kundenprojekte_Code_lokal/Code_pma/Dokumente_Kategorisierung/Dokumente/PoV-Kategorie-Split_Erste_und_Weitere-Seiten-gemischt/train\",\n",
    "    \"test\": \"/Users/christopherczaban/Local_Project_Files/Kundenprojekte_Code_lokal/Code_pma/Dokumente_Kategorisierung/Dokumente/PoV-Kategorie-Split_Erste_und_Weitere-Seiten-gemischt/test\"\n",
    "    }\n",
    "data_directory_train = data_directory['train']\n",
    "data_directory_test = data_directory['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5837e94-3075-493e-8fa5-6a23c6bbac4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFolder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset \u001b[38;5;28;01mas\u001b[39;00m hfds\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetDict\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset as hfds\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def load_imagefolder(data_dir):\n",
    "    image_folder_dataset = load_dataset(\"imagefolder\", data_dir)\n",
    "\n",
    "    images = [os.path.join(data_dir, path) for path, _ in image_folder_dataset.imgs]\n",
    "    labels = image_folder_dataset.targets\n",
    "\n",
    "    image_data = []\n",
    "    for image_path in images:\n",
    "        image = Image.open(image_path)\n",
    "        image_data.append(image)\n",
    "\n",
    "    data = {\"image\": image_data, \"image_path\": images, \"label\": labels}\n",
    "\n",
    "    return hfds.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "63dc2096-9a2f-4623-9c2b-257585cfb18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_path', 'label'],\n",
       "    num_rows: 85\n",
       "})"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = load_imagefolder(data_directory_train)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "e74cf80f-a8fe-4411-a4e6-954cd4a451f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_path', 'label'],\n",
       "    num_rows: 17\n",
       "})"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = load_imagefolder(data_directory_test)\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "7893267e-405d-41af-a8c7-359b352f84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\"train\": dataset_train, \"test\": dataset_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "9541d42c-3007-4259-b068-451f1a493cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'image_path', 'label'],\n",
       "        num_rows: 85\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'image_path', 'label'],\n",
       "        num_rows: 17\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee18c64-87e3-4ccc-9c31-3ffefc010eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = DatasetDict({\"test\": dataset_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73400a80-7e17-4b9b-81e1-3160681658f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir=data_directory, drop_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "497eb9e6-92b6-491f-b697-a031f723193a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 85\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 17\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376e9c8-5794-4dc6-b547-8104c214de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['train']['image'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d30bf-17cc-44fb-a7f8-3e530a594d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(path):\n",
    "    return Image.open(path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1a5c0-85be-464c-98ee-6167028d5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image\n",
    "path_to_img = \"/Users/christopherczaban/Local_Project_Files/Kundenprojekte_Code_lokal/Code_pma/Dokumente_Kategorisierung/Dokumente/PoV-Kategorie-Split_Erste_und_Weitere-Seiten-gemischt/train/ErsteSeiten/05.12.2022 - vn  668576245 - bauder robert ... - zur bearbeitung an vm(p015372975)_page_0.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168c27e-411a-4d1f-ad13-da239276cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(path_to_img)\n",
    "print(type(image))\n",
    "image = image.convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145da74-bb17-48c1-9b6b-72c36dec06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset['train'].features['label']\n",
    "labels = labels.names\n",
    "id2label = {v: k for v, k in enumerate(labels)}\n",
    "label2id = {k: v for v, k in enumerate(labels)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aded815-8505-4a5a-beca-5305098f78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get the label column for the train and test datasets\n",
    "train_labels = dataset['train']['label']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "# Count the number of instances for each label\n",
    "train_label_counts = Counter(train_labels)\n",
    "test_label_counts = Counter(test_labels)\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Train Label Counts:\")\n",
    "print(train_label_counts)\n",
    "print(\"\\nTest Label Counts:\")\n",
    "print(test_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896abfb-91ac-41c6-b657-e96986c1b9ff",
   "metadata": {},
   "source": [
    "## Creating a sampled dataset with n samples from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "8ea982c9-c617-47e1-a567-8ce5a802a015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_path', 'label'],\n",
       "    num_rows: 46\n",
       "})"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'label' data from 'train' subset of 'dataset' into a DataFrame\n",
    "dataset_df = pd.DataFrame(dataset['train']['label'], columns=['label'])\n",
    "\n",
    "# Group the DataFrame by the unique values in the 'label' column\n",
    "grouped_dataset_df = dataset_df.groupby('label')\n",
    "\n",
    "# Randomly sample 23 rows from each group in the DataFrame\n",
    "sample_dataset_df = grouped_dataset_df.sample(n=23, random_state=42)\n",
    "\n",
    "# Select examples from dataset according to sampled index\n",
    "dataset_sample_train = dataset['train'].select(sample_dataset_df.index)\n",
    "dataset_sample_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b36b2-0f76-4126-9621-e169fb84e066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "addd29e9-6991-4796-83b1-ef0ac87a0deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_path', 'label'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same steps as above for training data\n",
    "test_dataset_df = pd.DataFrame(dataset['test']['label'], columns=['label'])\n",
    "grouped_test_dataset_df = test_dataset_df.groupby('label')\n",
    "sample_test_dataset_df = grouped_test_dataset_df.sample(n=8, random_state=42)\n",
    "dataset_sample_test = dataset['test'].select(sample_test_dataset_df.index)\n",
    "dataset_sample_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3428c-3283-495a-8d6f-fdca1bba50f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bba825-8870-486d-beab-a55aec34f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LayoutLMv2ImageProcessor, LayoutLMv3ImageProcessor\n",
    "image_processor = LayoutLMv2ImageProcessor(ocr_lang='deu')\n",
    "#image_processor = LiLTImageProcessor()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ChrisPCz/lilt-roberta-base-wechsel-german\")\n",
    "#lm_tokenizer = AutoTokenizer.from_pretrained(\"nielsr/lilt-roberta-en-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a0fd7-4a0b-468f-86d7-61f239701663",
   "metadata": {},
   "source": [
    "# Application of OCR via custom process()-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346bbb0f-17fa-47d8-8500-f5c7fc373a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
    "\n",
    "\"\"\"\n",
    "features = Features({\n",
    "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'label': ClassLabel(num_classes=len(labels), names=labels),\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "# Define the features of the dataset\n",
    "features = Features({\n",
    "    'image': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'label': ClassLabel(num_classes=len(labels), names=labels),\n",
    "})\n",
    "\n",
    "# Function to normalize bounding boxes\n",
    "def normalize_bbox(bbox, width, height):\n",
    "    return [\n",
    "        int(1000 * (bbox[0] / width)),\n",
    "        int(1000 * (bbox[1] / height)),\n",
    "        int(1000 * (bbox[2] / width)),\n",
    "        int(1000 * (bbox[3] / height)),\n",
    "    ]\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(examples, normalize_boxes = False):\n",
    "    \n",
    "    images = examples['image']\n",
    "    \n",
    "    # Convert images to RGB format\n",
    "    images = [image.convert('RGB') for image in images]\n",
    "    \n",
    "    # Apply image processing to batch of images\n",
    "    batchFeature = image_processor(images)\n",
    "    \n",
    "    boxes = []\n",
    "    word_lists = []\n",
    "    for word_array, box_array in zip(batchFeature['words'], batchFeature['boxes']):\n",
    "        \n",
    "        bbox = []\n",
    "        \n",
    "        \n",
    "        for word, box in zip(word_array, box_array):\n",
    "            \n",
    "            # Normalize bounding box coordinates if required\n",
    "            #box = normalize_bbox(box, width, height) if normalize_boxes else box\n",
    "            \n",
    "            # Extend the bounding box for each word in the word_array\n",
    "            n_word_tokens = len(tokenizer.tokenize(word))\n",
    "            bbox.extend([box] * n_word_tokens)\n",
    "            \n",
    "        cls_box = sep_box = [0, 0, 0, 0]\n",
    "        \n",
    "        # Adds [cls_box] at the beginning and [sep_box] at the end\n",
    "        bbox = [cls_box] + bbox + [sep_box] # Gives a sequence length of 514 = 1 ([cls_box]) + 512 (bbox) + 1 ([sep_box])\n",
    "    \n",
    "        # Encode the word_array using tokenizer with truncation and padding\n",
    "        word_encoding = tokenizer(' '.join(word_array), truncation=True, max_length=512, padding=\"max_length\")\n",
    "        sequence_length = len(word_encoding.input_ids)\n",
    "        \n",
    "        # Trim the bbox to match the sequence length\n",
    "        bbox = bbox[:sequence_length]\n",
    "        \n",
    "        boxes.append(bbox)\n",
    "        word_lists.append(' '.join(word_array))\n",
    " \n",
    "    # Encode the word_lists using tokenizer with truncation and padding\n",
    "    batchEncoding = tokenizer(word_lists, truncation=True, max_length=512, padding=\"max_length\")#, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the sequence length from the input_ids tensor\n",
    "    sequence_length = torch.tensor(batchEncoding[\"input_ids\"]).shape[1]\n",
    "  \n",
    "    # Add additional fields to the batchEncoding dictionary\n",
    "    batchEncoding['image'] = batchFeature['pixel_values']\n",
    "    batchEncoding[\"label\"] = examples['label']\n",
    "    \n",
    "    # Create a list where each element 'boxes_example' in the 'boxes' list is concatenated with a repeated list '[[0, 0, 0, 0]]',\n",
    "    # ensuring it matches the desired 'sequence_length'.\n",
    "    # The resulting list is assigned to the 'bbox' key in the 'batchEncoding' dictionary.\n",
    "    batchEncoding[\"bbox\"] = [boxes_example + [[0, 0, 0, 0]] * (sequence_length - len(boxes_example)) for boxes_example in boxes]\n",
    "    \n",
    "    # convert to PyTorch\n",
    "    # batch = {k: torch.tensor(v, dtype=torch.int64) if isinstance(v[0], list) else v for k, v in batch.items()}\n",
    "    #batchEncoding = {k: torch.tensor(v) for k, v in batchEncoding.items()}\n",
    "    \n",
    "    return batchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfba1d5-134e-402a-a172-becc342ff909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36deef0c-758c-4309-95a4-5da09c39ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = dataset['train'].column_names\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d344e-99a0-41b9-8dad-d8e65c1a590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['train'].select([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cae198-e961-4199-a681-23d8afb72911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names of the 'train' subset\n",
    "column_names = dataset['train'].column_names\n",
    "\n",
    "# Print the column names\n",
    "print(column_names)\n",
    "\n",
    "# Apply the 'preprocess_data' function to the 'train' subset of 'dataset_sample_train' in a batched manner\n",
    "# with a batch size of 1. The 'features' variable specifies the expected structure of the processed data.\n",
    "# The 'remove_columns' argument removes the columns specified in 'column_names' from the resulting processed dataset.\n",
    "# The processed dataset is assigned to the 'proc_dataset_train' variable.\n",
    "proc_dataset_train = dataset_sample_train.map(preprocess_data, batched=True, batch_size=1, features=features, remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f177f7f-59e3-4b07-af97-d853611a7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat steps from above for test data\n",
    "proc_dataset_test = dataset_sample_test.map(preprocess_data, batched=True, batch_size=1, features=features, remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83de79-676d-4910-b133-eae7997a694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dataset_train = proc_dataset_train.remove_columns(['image'])\n",
    "proc_dataset_train = proc_dataset_train.rename_column('label','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5da78-fa67-4641-8a30-bbff317908a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dataset_test = proc_dataset_test.remove_columns(['image'])\n",
    "proc_dataset_test = proc_dataset_test.rename_column('label','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce0dba-0923-49b3-800c-3232dd4fa140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proc_dataset_train['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9563fe1-2b01-418d-ba63-2b42bf847ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proc_dataset_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cad69-7283-4a28-9857-a65954fa8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca948a82-fd86-41e3-b6f7-a01bd3ee1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, box in list(zip(proc_dataset_train['input_ids'][0], proc_dataset_train['bbox'][0])):\n",
    "    print(word,box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9545930-20e6-4782-ad7f-984e8d5c5188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79a93784-5b6b-4b9b-a5cb-6ccb64d605ee",
   "metadata": {},
   "source": [
    "Draw and check bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309efd02-4550-4c0a-86e3-2c9e7cb6cb9c",
   "metadata": {},
   "source": [
    "# Drawing and Check of Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6655e-e12c-4dd7-b0a7-cc812b7b2e85",
   "metadata": {},
   "source": [
    "The `draw_multi_boxes()` method is to compare bounding boxes of datasets with bounding boxes obtained through different methods, e.g. Tesseract and Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291b212-9af5-459d-a071-bf0c1dc39df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unnormalize_box(bbox, width, height):\n",
    "    return [\n",
    "        width * (bbox[0] / 1000),\n",
    "        height * (bbox[1] / 1000),\n",
    "        width * (bbox[2] / 1000),\n",
    "        height * (bbox[3] / 1000),\n",
    "    ]\n",
    "\n",
    "def draw_boxes(image, boxes, unnormalize_boxes = True):\n",
    "\n",
    "    width, height = image.size\n",
    "    print(width,height)\n",
    "    \n",
    "    boxes = [unnormalize_box(box, width, height) if unnormalize_boxes else box for box in boxes]\n",
    "\n",
    "    # draw predictions over the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, outline=\"red\")\n",
    "            \n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_multi_boxes(image, boxes1, boxes2, unnormalize_boxes = True):\n",
    "\n",
    "    width, height = image.size\n",
    "    print(width,height)\n",
    "    \n",
    "    boxes1 = [unnormalize_box(box, width, height) if unnormalize_boxes else box for box in boxes1]\n",
    "    boxes2 = [unnormalize_box(box, width, height) if unnormalize_boxes else box for box in boxes2]\n",
    "\n",
    "    # draw predictions over the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for box in boxes1:\n",
    "        draw.rectangle(box, outline=\"red\")\n",
    "        \n",
    "    for box in boxes2:\n",
    "        draw.rectangle(box, outline=\"blue\")\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437d9d9-4414-4507-a962-2cf8e9c47c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires image column, which was deleted from dataset above!\n",
    "idx = 0\n",
    "draw_boxes(dataset_sample_train['image'][idx], proc_dataset_train['bbox'][idx], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668279ca-86f4-40a8-9934-f40b0e7cbf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, box in list(zip(proc_dataset_train['input_ids'][0], proc_dataset_train['bbox'][0])):\n",
    "    print(tokenizer.decode(word),box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a2826-9389-4d00-afd8-164246e9ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(id2label[0])\n",
    "# Convert the token IDs back into words and print respective png\n",
    "print(tokenizer.decode(proc_dataset_train['input_ids'][0]))\n",
    "dataset_sample_train['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e8b0e-630f-4e5f-a910-e3462be393aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c1f92-81d6-4d32-b3d8-44e0129062d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dataset_train.set_format(type=\"torch\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07a341-b3ba-41eb-97d9-f843f45b8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dataset_test.set_format(type=\"torch\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ce173-53bb-4eaa-882b-fa047c5bde3a",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c12cb-d833-49df-9886-a5d5ca9262c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3fdff-eee7-45e4-8baf-1d322c9db795",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=len(list(label2id.keys()))\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdcfdc4-ec46-4dff-9881-e06184c755d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LiltForSequenceClassification, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Set random seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ChrisPCz/lilt-roberta-base-wechsel-german\", num_labels=len(list(label2id.keys())))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19468a81-e37a-43ec-9f97-42b3c84e886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7e37d-4e14-4588-9361-4b02cc636ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2c222-4a70-41b0-859a-46518a96056d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00fc41c-7efa-4795-94b7-035387fcf3f5",
   "metadata": {},
   "source": [
    "# Using HuggingFace-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d97b71-00d6-4a92-9d6f-e632ba790d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_predictions):\n",
    "    labels = eval_predictions.label_ids\n",
    "    preds = eval_predictions.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82765ce4-fb0f-43ba-a8f2-935338f6baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class ModelOutputCallback(TrainerCallback):\n",
    "    def __init__(self, trainer, eval_dataset):\n",
    "        self.trainer = trainer\n",
    "        self.eval_dataset = eval_dataset\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # get a few examples from the evaluation dataset\n",
    "        eval_examples = self.eval_dataset[:3]\n",
    "        \n",
    "        # convert the examples to a format suitable for the model\n",
    "        #encoded_eval_examples = self.trainer.tokenizer(eval_examples[\"image\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # move the inputs to the device\n",
    "        #device = self.trainer.args.device\n",
    "        #encoded_eval_examples = {k: v.to(device) for k, v in encoded_eval_examples.items()}\n",
    "   \n",
    "\n",
    "        eval_examples.remove_columns(['image','token_type_ids'])\n",
    "        # get the model output\n",
    "        model_output = self.trainer.model(**eval_examples)\n",
    "        predictions = model_output.logits.argmax(-1)\n",
    "        \n",
    "        # print the model output\n",
    "        print(\"Example inputs:\")\n",
    "        print(eval_examples)\n",
    "        print(\"Predictions:\")\n",
    "        print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d5f55-48fc-48ab-aaad-33642d2b25b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbaef2-5f98-4a4b-bf9d-6eb054a551a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15f5ecb4-9cb0-417f-855f-a65e92b13fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "output_dir = '/Users/christopherczaban/Local_Project_Files/LiLT_Model/LiLT_Roberta-base-german-wechsel_pma_classif_mixed_first_pages_and_further_pages'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=12,\n",
    "    #weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_dir=output_dir + \"/\" + \"train_logging\",\n",
    "    logging_steps=2,\n",
    "    #save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    use_mps_device=True\n",
    "    #greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876ce8d-6303-47a5-b57c-4c623434f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_proc_dataset_train = proc_dataset_train.shuffle(seed=41)\n",
    "shuffled_proc_dataset_test = proc_dataset_test.shuffle(seed=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52de892-e31d-431b-82be-57ec6d3ba007",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_proc_dataset_train.set_format(type=\"torch\", device=\"mps\")\n",
    "shuffled_proc_dataset_test.set_format(type=\"torch\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a03f68-2347-4289-8077-0a08b2ce5e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58318c1-dc88-471e-937f-a5a0d884d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shuffled_proc_dataset_train)\n",
    "print(shuffled_proc_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd055da-4592-4319-9cf4-b04e4e6d706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 1\n",
    "word_list = []\n",
    "for i,j in enumerate(shuffled_proc_dataset_train['input_ids'][ind][:]):\n",
    "    #print(i,j)\n",
    "    #print(i, tokenizer.decode(j.item()))\n",
    "    word_list.append(tokenizer.decode(j.item()))\n",
    "\n",
    "word_list_from_ger_ocr = \" \".join(word_list)\n",
    "\n",
    "print(\"index:\", ind)\n",
    "print(\"label:\", shuffled_proc_dataset_train['labels'][ind])\n",
    "print(word_list_from_ger_ocr)\n",
    "#dataset_sample_train['image'][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52cfcf-cd0d-4490-a240-c15c42790b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14c4fc-2690-4def-98fe-30945cb6ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=shuffled_proc_dataset_train,\n",
    "    eval_dataset=shuffled_proc_dataset_test,\n",
    "    #tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]#, ModelOutputCallback(trainer, shuffled_encoded_eval_dataset)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "326df83b-6011-45c0-8a21-88a7c3d0a6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherczaban/.envs/LiLT_m1/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/144 23:21, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.309800</td>\n",
       "      <td>0.994684</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.290300</td>\n",
       "      <td>0.244515</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.335772</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.100138</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.471453</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.521768</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.621991</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.649126</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.623752</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.599886</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.575672</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.572105</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=144, training_loss=0.1289556363281008, metrics={'train_runtime': 1410.8227, 'train_samples_per_second': 0.391, 'train_steps_per_second': 0.102, 'total_flos': 154540832440320.0, 'train_loss': 0.1289556363281008, 'epoch': 12.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f1583-3b7f-4293-a7b1-bb1f6498e05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11d46543-4179-4755-b69e-03bee510b144",
   "metadata": {},
   "source": [
    "# Create Feature Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "24d88c1b-a5c5-4d94-a03e-84cc901df01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that computes an embedding vector for an examples\n",
    "def create_compute_embedding_vector_function(model):\n",
    "    # Define the closure\n",
    "    def compute_embedding_vector(examples):\n",
    "\n",
    "        # Convert each inner list to a tensor\n",
    "        input_ids_tensors = [ids.clone().detach() for ids in examples['input_ids']]\n",
    "        # Stack the tensors\n",
    "        input_ids_stacked = torch.stack(input_ids_tensors)\n",
    "\n",
    "        # Repeat for attention masks\n",
    "        attention_mask_tensors = [mask.clone().detach() for mask in examples['attention_mask']]\n",
    "        attention_mask_stacked = torch.stack(attention_mask_tensors)\n",
    "\n",
    "        # Repeat for bboxes\n",
    "        bbox_tensors = [bbox.clone().detach() for bbox in examples['bbox']]\n",
    "        bbox_tensors_stacked = torch.stack(bbox_tensors)\n",
    "        \n",
    "        # Pack all inputs into a dictionary\n",
    "        inputs = {\n",
    "            'input_ids': input_ids_stacked,\n",
    "            'attention_mask': attention_mask_stacked,\n",
    "            'bbox': bbox_tensors_stacked,\n",
    "            'return_dict': True,\n",
    "            'output_hidden_states': True,\n",
    "        }\n",
    "        \n",
    "        device = torch.device('mps')  # Use 'cpu' for CPU\n",
    "\n",
    "        # Move all tensor items in the inputs dictionary to the device\n",
    "        inputs = {name: tensor.to(device) if torch.is_tensor(tensor) else tensor for name, tensor in inputs.items()}\n",
    "\n",
    "        # Call the model with the unpacked inputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Compute prediction\n",
    "        logits = outputs.logits\n",
    "        normalized_logits = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the index of the maximum logit for each example\n",
    "        predicted_class_indices = logits.argmax(-1)\n",
    "\n",
    "        # Convert logits and predicted_class_indices to lists\n",
    "        logits_list = logits.tolist()\n",
    "        normalized_logits_list = normalized_logits.tolist()\n",
    "        predicted_class_indices_list = predicted_class_indices.tolist()\n",
    "\n",
    "        # Now you can get the predicted class for each example\n",
    "        predicted_classes = [id2label[idx] for idx in predicted_class_indices_list]\n",
    "        \n",
    "        #last_hidden_states = outputs.hidden_states[-1]#.size()#[-1][0]#.shape#[1][0]\n",
    "        #CLS_token_embedding = last_hidden_states[:][0][:]\n",
    "        \n",
    "        # Assuming you have the tensor `last_hidden_states` with shape [batch_size, sequence_length, hidden_size]\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "        batch_size = last_hidden_states.size(0)\n",
    "        sequence_length = last_hidden_states.size(1)\n",
    "        hidden_size = last_hidden_states.size(2)\n",
    "\n",
    "        # Reshape the tensor to have shape [batch_size * sequence_length, hidden_size]\n",
    "        reshaped_hidden_states = last_hidden_states.view(batch_size * sequence_length, hidden_size)\n",
    "\n",
    "        # Extract embeddings from the first entry of each example\n",
    "        embeddings = reshaped_hidden_states[torch.arange(0, batch_size * sequence_length, sequence_length), :]\n",
    "\n",
    "        # Unbind the embeddings along the first dimension\n",
    "        embeddings_unbound = embeddings.unbind(0)\n",
    "\n",
    "        # Convert each tensor to a list\n",
    "        CLS_token_embeddings = [embedding.tolist() for embedding in embeddings_unbound]\n",
    "                 \n",
    "        return CLS_token_embeddings, predicted_classes, normalized_logits_list\n",
    "    \n",
    "        # Return the closure\n",
    "    return compute_embedding_vector\n",
    "\n",
    "# Create the closure\n",
    "compute_embedding_vector = create_compute_embedding_vector_function(trainer.model)\n",
    "\n",
    "# Define a function that adds an embedding vector to an examples\n",
    "def add_features(examples):\n",
    "    \n",
    "    # Compute the embedding vector for the examples\n",
    "    embedding_vector, predicted_classes, normalized_logits_list = compute_embedding_vector(examples)\n",
    "    # Add the embedding vector as a new feature\n",
    "    examples['embedding'] = embedding_vector\n",
    "    examples['predicted_classes'] = predicted_classes\n",
    "    examples['norm_logits'] = normalized_logits_list\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "b7619450-a0f1-48f8-8b90-85e2c740eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def create_feature_enriched_dataset(dataset, processed_dataset):\n",
    "    feature_enriched_dataset = processed_dataset.map(add_features, batched=True, batch_size=4)\n",
    "    feature_enriched_dataset = concatenate_datasets([feature_enriched_dataset, dataset], axis=1)\n",
    "    \n",
    "    gt_classes = [id2label[idx] for idx in feature_enriched_dataset['labels']]\n",
    "    feature_enriched_dataset = feature_enriched_dataset.add_column(\"gt_classes\",gt_classes)\n",
    "    \n",
    "    #TODO: Think about adding text for Phoenix analysis\n",
    "    #images = dataset['image']\n",
    "    # Convert images to RGB format\n",
    "    #images = [image.convert('RGB') for image in images]\n",
    "    # Apply image processing to batch of images\n",
    "    #batchFeature = image_processor(images)\n",
    "    \n",
    "    feature_enriched_dataset = feature_enriched_dataset.remove_columns(['labels','input_ids', 'attention_mask', 'bbox'])\n",
    "    \n",
    "    \n",
    "    return feature_enriched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249ff5a-8c94-43bf-87ed-abfd070508db",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dataset['image']\n",
    "# Convert images to RGB format\n",
    "images = [image.convert('RGB') for image in images]\n",
    "# Apply image processing to batch of images\n",
    "batchFeature = image_processor(images)\n",
    "for word_array in batchFeature['words']\n",
    "' '.join(word_array)\n",
    "batchEncoding = tokenizer(word_lists, truncation=True, max_length=512, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "64033fa4-95f5-4938-892f-12731594ab82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    47,  6457,  ...,     1,     1,     1],\n",
       "        [    0,    58, 22044,  ...,     1,     1,     1],\n",
       "        [    0, 10413,  1397,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0, 47020,   516,  ...,     1,     1,     1],\n",
       "        [    0, 32833,   448,  ...,     1,     1,     1],\n",
       "        [    0,    47,  6457,  ...,   395, 43818,     2]], device='mps:0')"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "26e0b1b9-8041-4ad1-82a4-3dbd59425c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "# Create a new dataset with the added feature\n",
    "#feature_enriched_test_dataset = proc_dataset_test.map(add_features, batched=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "d71e15e7-0896-4812-995a-3c6af82d6eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'bbox', 'embedding', 'predicted_classes', 'norm_logits', 'image', 'image_path', 'label', 'gt_classes'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_enriched_test_dataset = create_feature_enriched_dataset(dataset_sample_test, proc_dataset_test)\n",
    "feature_enriched_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "1b8c94dc-d8f4-48a4-ad06-07e56a70571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'bbox', 'embedding', 'predicted_classes', 'norm_logits', 'image', 'image_path', 'label', 'gt_classes'],\n",
       "    num_rows: 46\n",
       "})"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_enriched_train_dataset = create_feature_enriched_dataset(dataset_sample_train, proc_dataset_train)\n",
    "feature_enriched_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "7e3d66d4-9c2c-4815-aa5f-c6037062461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten WeitereSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n"
     ]
    }
   ],
   "source": [
    "for gt, pred in zip(feature_enriched_test_dataset['gt_classes'],feature_enriched_test_dataset['predicted_classes']):\n",
    "    print(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f5898841-b013-4850-8862-0ef9294b3f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten WeitereSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "ErsteSeiten ErsteSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n",
      "WeitereSeiten WeitereSeiten\n"
     ]
    }
   ],
   "source": [
    "for gt, pred in zip(feature_enriched_train_dataset['gt_classes'],feature_enriched_train_dataset['predicted_classes']):\n",
    "    print(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "10e39a4b-cc3d-4697-a63c-6684266743f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "feature_enriched_test_dataset = concatenate_datasets([feature_enriched_test_dataset, dataset_sample_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "49031e01-6cd8-4a15-877e-0f559bf1ebd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2480x3509>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2480x3509>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2480x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2480x3509>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2481x3508>]"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_enriched_test_dataset['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "74c5796b-a37d-4105-985e-a80f60feb96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'bbox', 'embedding', 'predicted_classes', 'norm_logits', 'image', 'image_path', 'label', 'gt_classes'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_enriched_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "3d9ac16f-31fb-4e68-ae59-acb14c1a31db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'bbox', 'embedding', 'predicted_classes', 'norm_logits', 'image', 'image_path', 'label', 'gt_classes'],\n",
       "    num_rows: 46\n",
       "})"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_enriched_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "62979bf7-6d3b-453f-9da4-b1b693aaa0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'bbox', 'embedding', 'predicted_classes', 'norm_logits', 'image', 'image_path', 'label', 'gt_classes'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_enriched_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "11c00e1b-4f70-4d29-aaf9-27c9d99e0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_enriched_train_df = pd.DataFrame(feature_enriched_train_dataset)\n",
    "feature_enriched_test_df = pd.DataFrame(feature_enriched_test_dataset)\n",
    "feature_enriched_train_df.drop(columns=['input_ids', 'attention_mask', 'bbox','image','label'], inplace=True)\n",
    "feature_enriched_test_df.drop(columns=['input_ids', 'attention_mask', 'bbox','image','label'], inplace=True)\n",
    "feature_enriched_train_df['prediction_score'] = feature_enriched_train_df['norm_logits'].apply(lambda x: x[0])\n",
    "feature_enriched_test_df['prediction_score'] = feature_enriched_test_df['norm_logits'].apply(lambda x: x[0])\n",
    "\n",
    "old_part = '/Users/christopherczaban/Local_Project_Files/Kundenprojekte_Code_lokal/Code_pma/Dokumente_Kategorisierung/Dokumente/PoV-Kategorie-Split_Erste_und_Weitere-Seiten-gemischt/'\n",
    "new_part = 'http://localhost:8000/'\n",
    "\n",
    "feature_enriched_train_df['image_path'] = feature_enriched_train_df['image_path'].str.replace(old_part, new_part)\n",
    "feature_enriched_test_df['image_path'] = feature_enriched_test_df['image_path'].str.replace(old_part, new_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "ce8a6dc9-360a-4aab-890f-a8a2d29f56bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['embedding', 'predicted_classes', 'norm_logits', 'image_path',\n",
      "       'gt_classes', 'prediction_score'],\n",
      "      dtype='object')\n",
      "Index(['embedding', 'predicted_classes', 'norm_logits', 'image_path',\n",
      "       'gt_classes', 'prediction_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(feature_enriched_train_df.columns)\n",
    "print(feature_enriched_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "2e7e2809-9a9a-4065-bfce-9e64cc0bd49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:8000/test/ErsteSeiten/12.07.2019 - vn  786979637 - bacinski anja ... - zur bearbeitung an vm(p011253367)_page_0.png'"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_enriched_test_df['image_path'][0]#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7c393-34e9-4c85-92d3-0314b4d2bb99",
   "metadata": {},
   "source": [
    "# Visualize and Analyze with Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "effe7666-9ba3-47b4-b831-d2d4ab6db720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "6e5d3b61-a796-4dc6-ad72-29bbc8738a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = px.Schema(\n",
    "    prediction_score_column_name=\"prediction_score\",\n",
    "    prediction_label_column_name=\"predicted_classes\",\n",
    "    actual_label_column_name=\"gt_classes\",\n",
    "    embedding_feature_column_names={\n",
    "        \"document_embedding\": px.EmbeddingColumnNames(\n",
    "            vector_column_name=\"embedding\",\n",
    "            link_to_data_column_name=\"image_path\"\n",
    "        ),\n",
    "    },\n",
    "    excluded_column_names=[\n",
    "        \"norm_logits\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "0f57dbc1-614b-4e7e-8598-5e1ce8b3db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.999468\n",
      "1     0.999352\n",
      "2     0.999476\n",
      "3     0.999408\n",
      "4     0.999476\n",
      "5     0.004691\n",
      "6     0.999357\n",
      "7     0.999324\n",
      "8     0.000460\n",
      "9     0.000453\n",
      "10    0.000538\n",
      "11    0.000615\n",
      "12    0.001982\n",
      "13    0.000576\n",
      "14    0.000957\n",
      "15    0.000531\n",
      "Name: prediction_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(feature_enriched_test_df['prediction_score'])\n",
    "#print(feature_enriched_train_df['prediction_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "7805c7d6-7b35-4f0d-b9e2-ec2f553dff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting items in column `embedding` to numpy.ndarray, because they have the following type: list\n",
      "converting items in column `embedding` to numpy.ndarray, because they have the following type: list\n"
     ]
    }
   ],
   "source": [
    "test_ds = px.Dataset(dataframe=feature_enriched_test_df, schema=schema, name=\"test\")\n",
    "train_ds = px.Dataset(dataframe=feature_enriched_train_df, schema=schema, name=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "8cec337d-bd4a-42ef-8150-8647dae2cb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 46 entries, 2023-07-12 00:15:55.942106+00:00 to 2023-07-12 00:15:55.942106+00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype              \n",
      "---  ------             --------------  -----              \n",
      " 0   embedding          46 non-null     object             \n",
      " 1   predicted_classes  46 non-null     object             \n",
      " 2   image_path         46 non-null     object             \n",
      " 3   gt_classes         46 non-null     object             \n",
      " 4   prediction_score   46 non-null     float64            \n",
      " 5   prediction_id      46 non-null     object             \n",
      " 6   timestamp          46 non-null     datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(5)\n",
      "memory usage: 2.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_ds.dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "09efc7ed-4947-4a22-a08e-a34b4b4e34d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 16 entries, 2023-07-12 00:15:55.912861+00:00 to 2023-07-12 00:15:55.912861+00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype              \n",
      "---  ------             --------------  -----              \n",
      " 0   embedding          16 non-null     object             \n",
      " 1   predicted_classes  16 non-null     object             \n",
      " 2   image_path         16 non-null     object             \n",
      " 3   gt_classes         16 non-null     object             \n",
      " 4   prediction_score   16 non-null     float64            \n",
      " 5   prediction_id      16 non-null     object             \n",
      " 6   timestamp          16 non-null     datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(5)\n",
      "memory usage: 1.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test_ds.dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "6fdb8465-21ee-4feb-84f9-6b8f64516707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To view the Phoenix app in your browser, visit http://localhost:57734/\n",
      " To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      " For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app(primary=test_ds, reference=train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "c52a6e2c-2979-4e67-9bf8-b1579ad9b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.active_session().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895249b7-175d-4a99-8871-bb4386f08604",
   "metadata": {},
   "source": [
    "# Custom Image Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "52e0e73a-d513-4dbf-be24-95f5493bc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers.utils import is_vision_available\n",
    "from transformers.utils.generic import TensorType\n",
    "\n",
    "from transformers.image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n",
    "from transformers.image_transforms import resize, to_channel_dimension_format, to_pil_image\n",
    "from transformers.image_utils import (\n",
    "    ChannelDimension,\n",
    "    ImageInput,\n",
    "    PILImageResampling,\n",
    "    infer_channel_dimension_format,\n",
    "    is_batched,\n",
    "    to_numpy_array,\n",
    "    valid_images,\n",
    ")\n",
    "\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "if is_vision_available():\n",
    "    import PIL\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def normalize_box(box, width, height):\n",
    "    return [\n",
    "        int(1000 * (box[0] / width)),\n",
    "        int(1000 * (box[1] / height)),\n",
    "        int(1000 * (box[2] / width)),\n",
    "        int(1000 * (box[3] / height)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def apply_az_ocr(image: PIL.PngImagePlugin.PngImageFile):\n",
    "\n",
    "    # apply OCR\n",
    "    words, boxes = apply_az_image_to_data(image)\n",
    "    \n",
    "    pil_image = to_pil_image(image)\n",
    "    image_width, image_height = pil_image.size\n",
    "\n",
    "    # finally, normalize the bounding boxes\n",
    "    normalized_boxes = []\n",
    "    for box in boxes:\n",
    "        normalized_boxes.append(normalize_box(box, image_width, image_height))\n",
    "\n",
    "    assert len(words) == len(normalized_boxes), \"Not as many words as there are bounding boxes\"\n",
    "\n",
    "    return words, normalized_boxes\n",
    "\n",
    "\n",
    "def flip_channel_order(image: np.ndarray, data_format: Optional[ChannelDimension] = None) -> np.ndarray:\n",
    "    input_data_format = infer_channel_dimension_format(image)\n",
    "    if input_data_format == ChannelDimension.LAST:\n",
    "        image = image[..., ::-1]\n",
    "    elif input_data_format == ChannelDimension.FIRST:\n",
    "        image = image[:, ::-1, ...]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported channel dimension: {input_data_format}\")\n",
    "\n",
    "    if data_format is not None:\n",
    "        image = to_channel_dimension_format(image, data_format)\n",
    "    return image\n",
    "\n",
    "\n",
    "class LiLTImageProcessor(BaseImageProcessor):\n",
    "    r\"\"\"\n",
    "    Constructs a LiLT image processor.\n",
    "    Args:\n",
    "        do_resize (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to resize the image's (height, width) dimensions to `(size[\"height\"], size[\"width\"])`. Can be\n",
    "            overridden by `do_resize` in `preprocess`.\n",
    "        size (`Dict[str, int]` *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n",
    "            Size of the image after resizing. Can be overridden by `size` in `preprocess`.\n",
    "        resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n",
    "            Resampling filter to use if resizing the image. Can be overridden by the `resample` parameter in the\n",
    "            `preprocess` method.\n",
    "        apply_ocr (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n",
    "            `apply_ocr` in `preprocess`.\n",
    "    \"\"\"\n",
    "\n",
    "    model_input_names = [\"pixel_values\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        do_resize: bool = True,\n",
    "        size: Dict[str, int] = None,\n",
    "        resample: PILImageResampling = PILImageResampling.BILINEAR,\n",
    "        apply_ocr: bool = True,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        size = size if size is not None else {\"height\": 224, \"width\": 224}\n",
    "        size = get_size_dict(size)\n",
    "\n",
    "        self.do_resize = do_resize\n",
    "        self.size = size\n",
    "        self.resample = resample\n",
    "        self.apply_ocr = apply_ocr\n",
    "\n",
    "    def resize(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        size: Dict[str, int],\n",
    "        resample: PILImageResampling = PILImageResampling.BILINEAR,\n",
    "        data_format: Optional[Union[str, ChannelDimension]] = None,\n",
    "        **kwargs\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Resize an image to `(size[\"height\"], size[\"width\"])`.\n",
    "        Args:\n",
    "            image (`np.ndarray`):\n",
    "                Image to resize.\n",
    "            size (`Dict[str, int]`):\n",
    "                Size of the output image.\n",
    "            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n",
    "                Resampling filter to use when resizing the image.\n",
    "            data_format (`str` or `ChannelDimension`, *optional*):\n",
    "                The channel dimension format of the image. If not provided, it will be the same as the input image.\n",
    "        \"\"\"\n",
    "        size = get_size_dict(size)\n",
    "        if \"height\" not in size or \"width\" not in size:\n",
    "            raise ValueError(f\"The size dictionary must contain the keys 'height' and 'width'. Got {size.keys()}\")\n",
    "        output_size = (size[\"height\"], size[\"width\"])\n",
    "        return resize(image, size=output_size, resample=resample, data_format=data_format, **kwargs)\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        images: ImageInput,\n",
    "        do_resize: bool = None,\n",
    "        size: Dict[str, int] = None,\n",
    "        resample: PILImageResampling = None,\n",
    "        apply_ocr: bool = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        data_format: ChannelDimension = ChannelDimension.FIRST,\n",
    "        **kwargs,\n",
    "    ) -> PIL.Image.Image:\n",
    "        \"\"\"\n",
    "        Preprocess an image or batch of images.\n",
    "        Args:\n",
    "            images (`ImageInput`):\n",
    "                Image to preprocess.\n",
    "            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n",
    "                Whether to resize the image.\n",
    "            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n",
    "                Desired size of the output image after resizing.\n",
    "            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n",
    "                Resampling filter to use if resizing the image. This can be one of the enum `PIL.Image` resampling\n",
    "                filter. Only has an effect if `do_resize` is set to `True`.\n",
    "            apply_ocr (`bool`, *optional*, defaults to `self.apply_ocr`):\n",
    "                Whether to apply the Azure OCR engine to get words + bounding boxes.\n",
    "            return_tensors (`str` or `TensorType`, *optional*):\n",
    "                The type of tensors to return. Can be one of:\n",
    "                    - Unset: Return a list of `np.ndarray`.\n",
    "                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n",
    "                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n",
    "                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n",
    "                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n",
    "            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n",
    "                The channel dimension format for the output image. Can be one of:\n",
    "                    - `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
    "                    - `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
    "        \"\"\"\n",
    "        do_resize = do_resize if do_resize is not None else self.do_resize\n",
    "        size = size if size is not None else self.size\n",
    "        size = get_size_dict(size)\n",
    "        resample = resample if resample is not None else self.resample\n",
    "        apply_ocr = apply_ocr if apply_ocr is not None else self.apply_ocr\n",
    "\n",
    "        if not is_batched(images):\n",
    "            images = [images]\n",
    "\n",
    "        if not valid_images(images):\n",
    "            raise ValueError(\n",
    "                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n",
    "                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n",
    "            )\n",
    "\n",
    "        if do_resize and size is None:\n",
    "            raise ValueError(\"Size must be specified if do_resize is True.\")\n",
    "\n",
    "       \n",
    "\n",
    "        if apply_ocr:\n",
    "            words_batch = []\n",
    "            boxes_batch = []\n",
    "            for image in images:\n",
    "                words, boxes = apply_az_ocr(image)\n",
    "                words_batch.append(words)\n",
    "                boxes_batch.append(boxes)\n",
    "                \n",
    "        # After azure ocr all transformations expect numpy arrays.\n",
    "        images = [to_numpy_array(image) for image in images]\n",
    "\n",
    "        if do_resize:\n",
    "            images = [self.resize(image=image, size=size, resample=resample) for image in images]\n",
    "\n",
    "        # flip color channels from RGB to BGR (as Detectron2 requires this)\n",
    "        images = [flip_channel_order(image) for image in images]\n",
    "        images = [to_channel_dimension_format(image, data_format) for image in images]\n",
    "\n",
    "        data = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n",
    "\n",
    "        if apply_ocr:\n",
    "            data[\"words\"] = words_batch\n",
    "            data[\"boxes\"] = boxes_batch\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b07f08-8a42-44cb-94b7-58f29661b3b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Azure Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ba011e41-8f11-4ea4-ade3-e33b1f3a1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_az_image_to_data(image):\n",
    "    \n",
    "    from io import BytesIO\n",
    "    \n",
    "    DocumentAnalysisClient = az_auth()\n",
    "    #poller = az_create_poller(path_to_sample_documents, DocumentAnalysisClient)\n",
    "\n",
    "\n",
    "    with BytesIO() as output:\n",
    "        image.save(output, format='PNG')\n",
    "        image_bytes = output.getvalue()\n",
    "\n",
    "\n",
    "    poller = DocumentAnalysisClient.begin_analyze_document(\n",
    "            \"prebuilt-document\", document=image_bytes\n",
    "        )\n",
    "\n",
    "    pages = az_get_pages_results_from_poller(poller)\n",
    "\n",
    "    document_word_list = []\n",
    "    document_bbox_list = []\n",
    "\n",
    "    for page in pages:\n",
    "        page_word_list, page_bbox_list = get_az_word_and_bboxes_from_page(page)\n",
    "        document_word_list.extend(page_word_list)\n",
    "        document_bbox_list.extend(page_bbox_list)\n",
    "\n",
    "    return document_word_list, document_bbox_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "6157093e-0448-4e8f-ae66-afbc405becb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def az_auth():\n",
    "    \n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "    endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"] = \"https://formulards.cognitiveservices.azure.com/\"\n",
    "    key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"] = \"b98c6924f8cb47e7aa8ff058194adb32\"\n",
    "\n",
    "    return DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "\n",
    "def az_create_poller(path_to_document, document_analysis_client):\n",
    "\n",
    "    with open(path_to_sample_documents, \"rb\") as f:\n",
    "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-read\", document=f)\n",
    "\n",
    "    return poller\n",
    "        \n",
    "\n",
    "def az_get_pages_results_from_poller(poller):\n",
    "    \n",
    "    return poller.result().pages\n",
    "    \n",
    "\n",
    "# Use function in loop through pages\n",
    "def get_az_word_and_bboxes_from_page(page):\n",
    "\n",
    "    lines = get_lines_from_page(page)\n",
    "    word_list = []\n",
    "    bbox_list = []\n",
    "    \n",
    "    for line in lines:\n",
    "        bboxes, words = get_words_and_bboxes_from_line(line)\n",
    "        if bboxes is not None:\n",
    "            word_list.extend(words)\n",
    "            bbox_list.extend(bboxes)\n",
    "        \n",
    "        \n",
    "    return word_list, bbox_list\n",
    "\n",
    "\n",
    "def get_lines_from_page(page):\n",
    "    return page.lines\n",
    "\n",
    "\n",
    "def get_words_and_bboxes_from_line(line):\n",
    "    \n",
    "    polygon = get_polygon_from_line(line)\n",
    "    hf_lilt_bbox = get_az_hf_lilt_bbox_from_polygon(polygon)\n",
    "    \n",
    "    if hf_lilt_bbox is not None:\n",
    "        words = get_words_from_line(line)\n",
    "        bbox_list = len(words) * [hf_lilt_bbox]\n",
    "        \n",
    "        word_list = []\n",
    "        [word_list.append(word.content) for word in words]\n",
    "        \n",
    "        return bbox_list, word_list\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_words_from_line(line):\n",
    "    return line.get_words()\n",
    "\n",
    "def get_polygon_from_line(line):\n",
    "    if not line.polygon:\n",
    "        return None\n",
    "    return line.polygon\n",
    "\n",
    "def get_az_hf_lilt_bbox_from_polygon(polygon):\n",
    "    \n",
    "    if polygon is None:\n",
    "        return None\n",
    "    \n",
    "    x_bbox = get_x_bbox(polygon)\n",
    "    x1l = x_bbox[0]\n",
    "    x2r = x_bbox[1]\n",
    "    x3r = x_bbox[2]\n",
    "    x4l = x_bbox[3]\n",
    "\n",
    "    y_bbox = get_y_bbox(polygon)\n",
    "    y1b = y_bbox[0]\n",
    "    y2b = y_bbox[1]\n",
    "    y3t = y_bbox[2]\n",
    "    y4t = y_bbox[3]\n",
    "\n",
    "    # h_position_embeddings = self.h_position_embeddings(bbox[:, :, 3] - bbox[:, :, 1])\n",
    "    # w_position_embeddings = self.w_position_embeddings(bbox[:, :, 2] - bbox[:, :, 0])\n",
    "    bw = x3r - x1l # x1l + bw = x1l + x3r - x1l\n",
    "    bh = y3t - y2b # y3t + bh = y3t + y3t - y2b = 2*y3t - y2b\n",
    "    # (left,top);(right,bottom)\n",
    "    #return [x1l,y3t,x3r,y2b]\n",
    "    \n",
    "    if x1l < x1l+bw and y2b < y2b+bh:\n",
    "        return [x1l,y2b,x1l+bw,y2b+bh]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_x_bbox(polygon):\n",
    "    if not polygon:\n",
    "        return []\n",
    "    else:\n",
    "        # left, right, right, left  \n",
    "        return [p.x for p in polygon]\n",
    "        \n",
    "def get_y_bbox(polygon):\n",
    "    if not polygon:\n",
    "        return []\n",
    "    else:\n",
    "        # bottom, bottom, top, top\n",
    "        return [p.y for p in polygon]\n",
    "\n",
    "\n",
    "def get_az_page_width(page):\n",
    "    return page.width\n",
    "\n",
    "def get_az_page_height(page):\n",
    "    return page.height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3b6aa-328b-4a5f-973e-a24f93b83c6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0cd19b6-9f1e-4e61-842e-2d1e03139cdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The Necessity for Processing the Bounding Boxes \"Manually\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01876549-e6b9-424c-8710-c19d454091cc",
   "metadata": {},
   "source": [
    "As it can be seen in the following, application of the tokenizer on the dataset will remove the bounding boxes. This is because the tokenizer XLMRobertaFast has no facility for processing bounding boxes. On the other handside, the tokenization processes increases the number of tokens, because the words a split up. Consequently, the bounding boxes have to be multiplied to have equal length sequences of input-tokens and bounding boxes. Therefore we need a method or a class to do this for us, i.e. which creates features ('input_ids', 'attention_mask', 'bbox') , which we will see in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1f3a1-7c1d-47a9-b6aa-003b292149d3",
   "metadata": {},
   "source": [
    "image: <class 'numpy.ndarray'>\n",
    "gray: <class 'numpy.ndarray'>\n",
    "thresh: <class 'float'>\n",
    "gray: <class 'numpy.ndarray'>\n",
    "opening: <class 'numpy.ndarray'>\n",
    "canny: <class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec8d19-3f64-418f-8767-af1bf222c977",
   "metadata": {},
   "source": [
    "The images (pixel values) obtained through the image processor have a size of 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62bd70-7433-42f1-8469-c8af0ead86ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da575a6-4723-4001-b543-c65cb9e61001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_bbox(bbox, width, height):\n",
    "    return [\n",
    "        int(1000 * (bbox[0] / width)),\n",
    "        int(1000 * (bbox[1] / height)),\n",
    "        int(1000 * (bbox[2] / width)),\n",
    "        int(1000 * (bbox[3] / height)),\n",
    "    ]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataset, tokenizer, normalize_boxes = False):\n",
    "    self.dataset = dataset\n",
    "    self.tokenizer = tokenizer\n",
    "    self.normalize_boxes = normalize_boxes\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # get item\n",
    "    example = self.dataset[idx]\n",
    "    image = example[\"images\"]\n",
    "    #labels = example[\"labels\"]\n",
    "    words = example[\"words\"]\n",
    "    boxes = example[\"boxes\"]\n",
    "\n",
    "    # prepare for the model\n",
    "    width, height = image.size\n",
    "\n",
    "    bbox = []\n",
    "    for word, box in zip(words, boxes):\n",
    "        box = normalize_bbox(box, width, height) if self.normalize_boxes else box\n",
    "        #print(box)\n",
    "        n_word_tokens = len(tokenizer.tokenize(word))\n",
    "        bbox.extend([box] * n_word_tokens)\n",
    "    #print(bbox)\n",
    "    cls_box = sep_box = [0, 0, 0, 0]\n",
    "    bbox = [cls_box] + bbox + [sep_box]\n",
    "\n",
    "    encoding = self.tokenizer(\" \".join(words), truncation=True, max_length=512, padding='max_length')\n",
    "    sequence_length = len(encoding.input_ids)\n",
    "    #print(sequence_length)\n",
    "    # truncate boxes and labels based on length of input ids\n",
    "    bbox = bbox[:sequence_length] \n",
    "\n",
    "    encoding[\"bbox\"] = bbox\n",
    "    encoding[\"labels\"] = example[\"labels\"]\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37411b83-e880-428a-93d0-f5ca00fe243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = CustomDataset(proc_dataset_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af5efa-a1a4-4903-8217-ff15ff7d95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288bc16-88ae-4828-80b9-8d6763803f3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decode and Check `input_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032857f5-b26e-4c8a-b9e2-58623527f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, box in list(zip(features_train[0]['input_ids'],features_train[0]['bbox']))[0:20]:\n",
    "    print(tokenizer.decode([id]),box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3906f7-4584-4b8f-ab1b-9204fab09204",
   "metadata": {},
   "source": [
    "# Define Pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8b5b4-cdbf-427e-8f54-addc74a3575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in proc_dataset_train.features:\n",
    "    print(feature)\n",
    "    print(proc_dataset_train[feature].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd339b39-61ba-44a9-8634-b2e3063d8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(proc_dataset_train, batch_size=2)\n",
    "batch_train = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02ff5e-8826-448e-b306-9f03c6cd059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in batch_train.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8467d-fffe-4c13-b0c2-e0e96596e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(proc_dataset_test, batch_size=2)\n",
    "batch_test = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbe6f6-8e16-426f-982f-629872f973f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in batch_test.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b930b-7cee-48e8-8e58-48c876eef2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "We need a dataloader to load our features we obtained from our `CustomDataset`-Class and to provide them to the model during training. To accomplish this we need a data collator method serves as batch feature provider of the DataLoader object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8ff22-32d9-4e9b-adbc-10368b9044d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check dimensions of feature batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677d670-e792-486e-a60a-608b2eeaad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Checking decoded `input_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1869e-1557-4020-a386-8f970c24e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(batch_test[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf69b88-6987-46db-9409-e7390a6f251b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f98e5384-906d-463c-a27c-8908193e5ec1",
   "metadata": {},
   "source": [
    "# Training with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2102c-660a-4318-a060-9d8cac80c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = len(dataset_sample_train)\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52743dc9-031c-4212-aa34-cfb534c7aa97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "#  lr=0.00001, batch_size=1, nr samples: 8 # <-- bad parameters\n",
    "#  lr=0.0001, batch_size=1, nr samples: 8 # <-- bad parameters\n",
    "#  lr=0.0005, batch_size=1, nr samples: 8 # <-- bad parameters\n",
    "#  lr=0.001, batch_size=1, nr samples: 8 # <-- bad parameters\n",
    "#  lr=0.0001, batch_size=2, nr samples: 8 # <-- good parameters\n",
    "#  lr=0.0005, batch_size=2, nr samples: 8 # <-- bad parameters\n",
    "#  lr=0.0001, batch_size=4, nr samples: 8 # <-- bad parameters\n",
    "\n",
    "# Set random seed for optimizer\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(torch.get_default_dtype())\n",
    "            torch.manual_seed(42)\n",
    "            state[k].copy_(torch.randn_like(v))\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "num_train_epochs = 20\n",
    "# t_total seems to be irrelevant\n",
    "# t_total = len(train_dataloader) * num_train_epochs # total number of training steps \n",
    "\n",
    "#put the model in training mode\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch in tqdm(dataloader_train):\n",
    "        # forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predictions = outputs.logits.argmax(-1)\n",
    "        correct += (predictions == batch['labels']).float().sum()\n",
    "        print(predictions)\n",
    "        print(correct)\n",
    "\n",
    "        # backward pass to get the gradients \n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "    print(\"Loss:\", running_loss / batch[\"input_ids\"].shape[0])\n",
    "    accuracy = 100 * correct / len_data\n",
    "    print(\"Training accuracy:\", accuracy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f87a0-da04-4cc8-b7c9-feb7c205b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c05f6d-9313-4d96-8da9-7d608bf27bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set the initial learning rate\n",
    "lr = 5e-5\n",
    "num_train_epochs = 15\n",
    "global_step = 0\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "total_steps = len(dataloader) * num_train_epochs\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "print(\"warmup_steps: \", warmup_steps)\n",
    "\n",
    "scheduler_after_step = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "# put the model in training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        # forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predictions = outputs.logits.argmax(-1)\n",
    "        correct += (predictions == batch['labels']).float().sum()\n",
    "\n",
    "        # backward pass to get the gradients \n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        if global_step < warmup_steps:\n",
    "            print(\"step: \", step)\n",
    "            lr_this_step = lr * (global_step / warmup_steps)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "                print(\"lr_this_step:\", lr_this_step)\n",
    "\n",
    "        else:\n",
    "            scheduler_after_step.step()\n",
    "            \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / len_data\n",
    "    print(f\"Epoch {epoch} - Loss: {avg_loss:.4f} - Training accuracy: {accuracy.item():.2f}%\")\n",
    "    \n",
    "    # Evaluate on validation set and adjust learning rate\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "    \"\"\" \n",
    "    scheduler.step(avg_loss)\n",
    "    #scheduler.step(val_loss)\n",
    "    #print(f\"Validation Loss: {val_loss:.4f} - Validation accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    model.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
